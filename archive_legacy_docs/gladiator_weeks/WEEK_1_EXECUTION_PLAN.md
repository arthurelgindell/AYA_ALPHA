# GLADIATOR WEEK 1 EXECUTION PLAN
## Data Preparation & Network Upgrade

**Date**: October 23-29, 2025 (7 days)  
**Status**: âœ… **APPROVED TO START** (Week 0 GO decision)  
**Phase**: Week 1 of 8  
**Prerequisites**: âœ… Week 0 Reality Check v2.0 PASSED

---

## WEEK 1 OVERVIEW

**Primary Objectives**:
1. Install 10GbE network between ALPHA and BETA
2. Launch dataset expansion (Track 3) on BETA in parallel
3. Prepare infrastructure for Week 2-3 Blue Team training
4. Validate network performance and data transfer capabilities

**Success Criteria**:
- 10GbE network operational with â‰¥500 MB/s throughput
- Dataset expansion launched and progressing
- Infrastructure ready for 10K+ sample training
- Week 1 tasks completed on schedule

**Timeline**: 7 days (October 23-29, 2025)

---

## TASK BREAKDOWN

### DAY 1: Wednesday, October 23

**TASK 14: 10GbE Network Installation** ðŸ”´ CRITICAL
```
Duration: 4-6 hours
System: ALPHA + BETA
Owner: Arthur

Description:
Install and configure 10GbE network connection between ALPHA and BETA
Mac Studio systems for high-speed data transfer.

Hardware Requirements:
â”œâ”€ 10GbE Thunderbolt adapters (2Ã—) or built-in 10GbE
â”œâ”€ Cat 6a or Cat 7 Ethernet cable
â”œâ”€ 10GbE network switch (if not direct connection)
â””â”€ Thunderbolt 4 cables (if using adapters)

Installation Steps:
1. Connect 10GbE adapters to both systems
2. Configure network interfaces
3. Set static IP addresses (10.0.10.1 for ALPHA, 10.0.10.2 for BETA)
4. Test connectivity (ping, iperf3)
5. Measure throughput

Success Criteria:
â”œâ”€ Network connectivity: PASS (ping successful)
â”œâ”€ Throughput: â‰¥500 MB/s (measured with iperf3)
â”œâ”€ Latency: <1ms
â””â”€ Stability: No packet loss over 10-minute test

Verification:
# On ALPHA
sudo ifconfig en[X] 10.0.10.1 netmask 255.255.255.0
ping 10.0.10.2

# Throughput test (run iperf3 server on BETA)
iperf3 -c 10.0.10.2 -t 60

Evidence Required:
â”œâ”€ iperf3 output showing â‰¥500 MB/s
â”œâ”€ Network configuration screenshots
â””â”€ Stability test results (0% packet loss)
```

**TASK 15: Launch Dataset Expansion (Track 3)** ðŸ”´ CRITICAL
```
Duration: 2-4 hours (setup), then 2-3 weeks (generation)
System: BETA
Owner: Claude / Arthur

Description:
Launch parallel dataset expansion process on BETA to generate 11,000
high-quality samples (5,500 attacks + 5,500 benign) while Week 1-2
infrastructure work proceeds.

Prerequisites:
â”œâ”€ BETA system accessible
â”œâ”€ red_combat container running
â”œâ”€ LM Studio operational (http://localhost:1234)
â”œâ”€ Expansion plan ready (datasets/expansion_plan_track3.json)
â””â”€ Storage available (â‰¥50 GB free on /Volumes/DATA)

Phase 1 Setup:
1. Verify BETA system status
2. Check LM Studio model availability
3. Create generation scripts
4. Set up automated monitoring
5. Launch first batch (privilege escalation - 800 samples)

Success Criteria (Setup):
â”œâ”€ Generation scripts created and tested
â”œâ”€ First batch launched (privilege escalation)
â”œâ”€ Monitoring system operational
â””â”€ Progress tracking established

Verification:
ssh beta.local "docker ps | grep red_combat"
ssh beta.local "curl http://localhost:1234/v1/models"
ssh beta.local "ls -lh /Volumes/DATA/GLADIATOR/datasets/expansion/"

Evidence Required:
â”œâ”€ First batch initiated (log file)
â”œâ”€ Sample quality verified (manual review of first 10)
â””â”€ Timeline established (samples per day estimate)
```

### DAY 2: Thursday, October 24

**TASK 16: Network Performance Validation** â³ HIGH
```
Duration: 2-3 hours
System: ALPHA + BETA
Owner: Arthur

Description:
Validate 10GbE network performance with actual data transfer scenarios
matching Week 2-3 Blue Team training requirements.

Test Scenarios:
1. Large file transfer (10 GB test file)
2. Multiple concurrent transfers
3. Sustained throughput test (1 hour)
4. Network stability under load

Success Criteria:
â”œâ”€ Single file transfer: â‰¥500 MB/s
â”œâ”€ Concurrent transfers: â‰¥400 MB/s (2 streams)
â”œâ”€ Sustained performance: â‰¥450 MB/s over 1 hour
â”œâ”€ Packet loss: <0.01%
â””â”€ Latency: <1ms

Verification:
# Create 10GB test file
dd if=/dev/urandom of=/tmp/test_10gb.dat bs=1m count=10240

# Transfer test
time rsync -avh --progress /tmp/test_10gb.dat beta.local:/tmp/

Evidence Required:
â”œâ”€ Transfer speed measurements
â”œâ”€ Throughput graphs (if available)
â””â”€ Stability test logs
```

**TASK 17: Monitor Dataset Expansion Progress** â³ ONGOING
```
Duration: 2-3 weeks (check daily)
System: BETA
Owner: Claude / Arthur

Description:
Monitor Track 3 dataset expansion progress, quality, and timeline.

Daily Checks:
â”œâ”€ Samples generated count
â”œâ”€ Quality spot-check (10 random samples)
â”œâ”€ Category distribution
â”œâ”€ Error rate
â””â”€ Timeline projection

Progress Targets:
â”œâ”€ Day 2: 200-400 samples (privilege escalation)
â”œâ”€ Day 4: 600-800 samples (privilege escalation complete)
â”œâ”€ Week 1 End: 1,500-2,000 samples (privilege + buffer overflow)
â”œâ”€ Week 2 End: 5,000-7,000 samples
â””â”€ Week 3 End: 11,000+ samples COMPLETE

Success Criteria:
â”œâ”€ Daily progress â‰¥200 samples
â”œâ”€ Quality rate â‰¥90% (manual review)
â”œâ”€ Category diversity maintained
â””â”€ On track for 3-week completion
```

### DAY 3-4: Friday-Saturday, October 25-26

**TASK 18: Prepare Blue Team Training Infrastructure** â³ HIGH
```
Duration: 8-12 hours
System: ALPHA
Owner: Claude / Arthur

Description:
Prepare ALPHA system for large-scale Blue Team training (Week 2-3).

Infrastructure Preparation:
1. Storage allocation (â‰¥100 GB for datasets)
2. Training directory structure
3. Checkpoint storage strategy (RAID or redundant)
4. Monitoring dashboard setup
5. Resource allocation planning

Directory Structure:
/Users/arthurdell/GLADIATOR/
â”œâ”€ datasets/
â”‚   â”œâ”€ blue_team_training/
â”‚   â”‚   â”œâ”€ train.jsonl (80% of 10K = 8,000 samples)
â”‚   â”‚   â”œâ”€ valid.jsonl (20% of 10K = 2,000 samples)
â”‚   â”‚   â””â”€ test.jsonl (separate test set)
â”‚   â””â”€ metadata/
â”œâ”€ checkpoints/
â”‚   â””â”€ blue_team_8b/
â”‚       â”œâ”€ iteration_checkpoints/
â”‚       â””â”€ final/
â””â”€ logs/
    â””â”€ blue_team_training/

Success Criteria:
â”œâ”€ Storage allocated: â‰¥100 GB
â”œâ”€ Directory structure created
â”œâ”€ Monitoring system operational
â”œâ”€ Resource plan documented
â””â”€ Ready for 10K sample training

Verification:
df -h /Users/arthurdell/GLADIATOR
ls -la /Users/arthurdell/GLADIATOR/datasets/blue_team_training/

Evidence Required:
â”œâ”€ Directory structure screenshot
â”œâ”€ Storage allocation proof
â””â”€ Monitoring dashboard operational
```

**TASK 19: Quality Review - First Dataset Expansion Batch** â³ HIGH
```
Duration: 3-4 hours
System: BETA â†’ ALPHA
Owner: Arthur / Claude

Description:
Review first batch of expanded dataset (privilege escalation ~800 samples)
for quality, accuracy, and formatting.

Review Process:
1. Transfer first batch from BETA to ALPHA
2. Manual review of 80 samples (10%)
3. Automated validation (format, length, content)
4. Label accuracy check
5. Approve or request corrections

Quality Criteria:
â”œâ”€ Label accuracy: â‰¥95% (manual review)
â”œâ”€ Format valid: 100% (automated check)
â”œâ”€ Content quality: â‰¥90% (coherent, realistic)
â”œâ”€ Diversity: All 8 privilege escalation techniques represented
â””â”€ No duplicates: <1% similarity to existing samples

Success Criteria:
â”œâ”€ 80 samples manually reviewed
â”œâ”€ Quality metrics meet thresholds
â”œâ”€ Batch approved for inclusion
â””â”€ Corrections identified (if needed)

Evidence Required:
â”œâ”€ Review checklist (80 samples)
â”œâ”€ Quality metrics report
â””â”€ Approval decision documented
```

### DAY 5-6: Sunday-Monday, October 27-28

**TASK 20: Data Transfer Protocol Setup** â³ MEDIUM
```
Duration: 4-6 hours
System: ALPHA + BETA
Owner: Claude

Description:
Create automated data transfer protocol for moving expanded dataset
from BETA to ALPHA when ready.

Components:
1. Automated sync script (rsync over 10GbE)
2. Checksum verification
3. Progress monitoring
4. Error handling and retry logic
5. Transfer completion notification

Script Features:
â”œâ”€ Incremental sync (only new files)
â”œâ”€ MD5 checksum verification
â”œâ”€ Bandwidth limiting (if needed)
â”œâ”€ Logging and monitoring
â””â”€ Automatic retry on failure

Success Criteria:
â”œâ”€ Script created and tested
â”œâ”€ Test transfer â‰¥500 MB/s
â”œâ”€ Checksum verification works
â”œâ”€ Error handling tested
â””â”€ Ready for production use

Verification:
# Test with sample dataset
./scripts/sync_beta_to_alpha.sh --test

Evidence Required:
â”œâ”€ Script file (sync_beta_to_alpha.sh)
â”œâ”€ Test transfer log
â””â”€ Checksum verification proof
```

### DAY 7: Tuesday, October 29

**TASK 21: Week 1 Completion Review** ðŸ”´ CRITICAL
```
Duration: 2-3 hours
System: ALPHA
Owner: Claude / Arthur

Description:
Comprehensive review of Week 1 accomplishments and readiness for Week 2-3.

Review Checklist:
â”œâ”€ 10GbE network: Operational at â‰¥500 MB/s
â”œâ”€ Dataset expansion: On track (â‰¥2,000 samples by Week 1 end)
â”œâ”€ Quality review: First batch approved
â”œâ”€ Infrastructure: Ready for Blue Team training
â”œâ”€ Data transfer: Protocol tested and operational
â””â”€ Timeline: On track for Week 2-3 start

Deliverables:
â”œâ”€ Week 1 completion report
â”œâ”€ Network performance report
â”œâ”€ Dataset expansion progress report
â”œâ”€ Week 2-3 readiness checklist
â””â”€ GO/NO-GO decision for Week 2-3

Success Criteria:
â”œâ”€ All Week 1 tasks complete
â”œâ”€ No critical blockers
â”œâ”€ Infrastructure validated
â””â”€ Ready to proceed

Decision Gate:
IF all criteria met: GO to Week 2-3 (Blue Team Training)
IF blockers exist: Address before proceeding
```

---

## PARALLEL EXECUTION: DATASET EXPANSION (TRACK 3)

**Timeline**: 2-3 weeks (Oct 23 - Nov 13)  
**System**: BETA  
**Owner**: Claude / Arthur (rotating)

### Week 1 Target (Oct 23-29): 2,000 samples
```
Priority Batches:
â”œâ”€ Privilege Escalation: 800 samples (Day 1-4)
â”œâ”€ Buffer Overflow: 600 samples (Day 4-6)
â””â”€ Path Traversal: 500 samples (Day 6-7)

Daily Target: ~285 samples/day
Quality Gate: Manual review 10% per batch
```

### Week 2 Target (Oct 30 - Nov 5): 4,000 samples (cumulative 6,000)
```
Medium Priority Batches:
â”œâ”€ SQL Injection: 600 samples
â”œâ”€ XSS: 600 samples
â”œâ”€ Command Injection: 500 samples
â”œâ”€ Phishing: 500 samples
â”œâ”€ DoS: 400 samples
â””â”€ Malware: 400 samples

Daily Target: ~570 samples/day
Quality Gate: Automated validation 100%, manual 5%
```

### Week 3 Target (Nov 6-13): 5,000 samples (cumulative 11,000)
```
Final Batches:
â”œâ”€ MITM: 300 samples
â”œâ”€ Malware (continued): 300 samples
â”œâ”€ Benign samples: 4,400 samples
â””â”€ Additional attacks: 1,000 samples (to reach 5,500)

Daily Target: ~715 samples/day
Quality Gate: Final review 100 random samples
Format Conversion: All to chat template
```

---

## RESOURCE REQUIREMENTS

### Network Hardware
```
Equipment:
â”œâ”€ 10GbE Thunderbolt adapter Ã— 2: $200-300 each
â”œâ”€ Cat 6a cable (appropriate length): $20-50
â””â”€ Optional: 10GbE switch: $500-1000 (if not direct)

Total Cost: ~$500-1,500
ROI: 5-10Ã— faster data transfer, critical for timeline
```

### Storage
```
ALPHA:
â”œâ”€ Dataset storage: ~50 GB (11K samples)
â”œâ”€ Checkpoint storage: ~500 GB (training checkpoints)
â”œâ”€ Log storage: ~10 GB
â””â”€ Total needed: ~560 GB (available: 14+ TB âœ…)

BETA:
â”œâ”€ Generation workspace: ~100 GB
â”œâ”€ Raw patterns: ~50 GB
â””â”€ Total needed: ~150 GB (available: 16 TB âœ…)
```

### Compute (Dataset Generation on BETA)
```
LM Studio:
â”œâ”€ Model: Foundation-Sec-8B or similar
â”œâ”€ Throughput: ~40 tokens/second
â”œâ”€ Samples per hour: ~15-20 (depending on complexity)
â””â”€ Daily capacity: 300-400 samples (with automation)

Resource Monitoring:
â”œâ”€ RAM usage: ~50-100 GB (within 512 GB capacity)
â”œâ”€ GPU utilization: ~60-80% (acceptable)
â””â”€ Storage writes: ~2-5 GB/day
```

---

## SUCCESS METRICS

### Network Performance
```
Metric                    | Target    | Measurement Method
--------------------------|-----------|-------------------
Throughput (single)       | â‰¥500 MB/s | iperf3
Throughput (concurrent)   | â‰¥400 MB/s | parallel rsync
Latency                   | <1 ms     | ping statistics
Packet loss               | <0.01%    | long-duration ping
Stability                 | 24h uptime| continuous monitor
```

### Dataset Expansion (Week 1 Portion)
```
Metric                    | Target       | Measurement Method
--------------------------|--------------|-------------------
Samples generated         | â‰¥2,000       | file count
Quality rate              | â‰¥90%         | manual review
Category distribution     | Balanced     | automated analysis
Label accuracy            | â‰¥95%         | manual verification
Duplicate rate            | <1%          | similarity check
```

### Infrastructure Readiness
```
Metric                    | Status   | Verification Method
--------------------------|----------|--------------------
Storage allocated         | â‰¥100 GB  | df -h
Network operational       | Yes      | iperf3 + ping
Transfer protocol ready   | Yes      | test transfer
Monitoring operational    | Yes      | dashboard check
Ready for Week 2-3        | Yes      | checklist review
```

---

## RISK MITIGATION

### Risk 1: 10GbE Installation Issues
**Probability**: Medium  
**Impact**: High (delays Week 2-3)

Mitigation:
â”œâ”€ Backup plan: Continue with 1GbE (slower but functional)
â”œâ”€ Alternative: USB 3.2 Gen 2Ã—2 (20 Gbps) for data transfer
â”œâ”€ Contingency: Allow 1-2 extra days for troubleshooting
â””â”€ Escalation: Engage network specialist if needed

### Risk 2: Dataset Expansion Behind Schedule
**Probability**: Medium  
**Impact**: Medium (can adjust Week 2-3 start)

Mitigation:
â”œâ”€ Daily monitoring and adjustment
â”œâ”€ Minimum viable: 5,000 samples (vs 11,000 target)
â”œâ”€ Automation: Increase batch size if needed
â”œâ”€ Parallel generation: Use multiple LLM instances
â””â”€ Timeline flex: Week 2-3 can start with partial dataset

### Risk 3: Quality Issues in Generated Samples
**Probability**: Low  
**Impact**: High (bad training data = bad model)

Mitigation:
â”œâ”€ Manual review 10% of samples per batch
â”œâ”€ Automated validation 100% of samples
â”œâ”€ Reject low-quality samples immediately
â”œâ”€ Regenerate if batch quality <90%
â””â”€ Expert review for ambiguous samples

---

## MONITORING & REPORTING

### Daily Status Updates
```
Format:
================================================================================
WEEK 1 DAY [X] STATUS - [DATE]
================================================================================

Network Status:
â”œâ”€ 10GbE Installation: [COMPLETE / IN PROGRESS / NOT STARTED]
â”œâ”€ Throughput: [XXX MB/s] (Target: â‰¥500 MB/s)
â””â”€ Status: [OPERATIONAL / ISSUES / NOT READY]

Dataset Expansion (Track 3):
â”œâ”€ Total Samples: [X,XXX] / 11,000
â”œâ”€ Today Generated: [XXX] samples
â”œâ”€ Quality Rate: [XX]% (Target: â‰¥90%)
â”œâ”€ Current Batch: [Category name]
â””â”€ Status: [ON TRACK / BEHIND / AHEAD]

Week 1 Progress:
â”œâ”€ Tasks Complete: [X] / 8
â”œâ”€ Days Elapsed: [X] / 7
â”œâ”€ Timeline: [ON TRACK / AT RISK / DELAYED]
â””â”€ Blockers: [NONE / list if any]

Next 24 Hours:
[Planned tasks]

================================================================================
```

### Weekly Summary (End of Week 1)
```
Required content:
â”œâ”€ All tasks completed (Y/N with evidence)
â”œâ”€ Network performance validated
â”œâ”€ Dataset expansion progress
â”œâ”€ Infrastructure readiness assessment
â”œâ”€ Week 2-3 GO/NO-GO decision
â””â”€ Evidence files and measurements
```

---

## WEEK 1 DELIVERABLES

### Infrastructure
1. âœ… 10GbE network operational (â‰¥500 MB/s verified)
2. âœ… Data transfer protocol tested and documented
3. âœ… Storage allocated and organized
4. âœ… Monitoring system operational

### Data
1. âœ… Dataset expansion launched (Track 3)
2. âœ… First 2,000 samples generated (privilege escalation + buffer overflow)
3. âœ… Quality review completed on first batch
4. âœ… On track for 11,000 total by end of Week 3

### Documentation
1. âœ… Week 1 completion report
2. âœ… Network performance report
3. âœ… Dataset expansion progress report
4. âœ… Week 2-3 readiness checklist

---

## TRANSITION TO WEEK 2-3

### Prerequisites for Week 2-3 Start
```
MANDATORY:
â”œâ”€ 10GbE network operational
â”œâ”€ Dataset expansion â‰¥5,000 samples (minimum viable)
â”œâ”€ Infrastructure ready (storage, monitoring)
â””â”€ Week 1 completion approved

OPTIMAL:
â”œâ”€ Dataset expansion â‰¥10,000 samples (full target)
â”œâ”€ All quality reviews complete
â”œâ”€ Network performance validated
â””â”€ No outstanding issues
```

### Week 2-3 Preview
```
Task: Blue Team Training (Foundation-Sec-8B full-scale fine-tuning)
Duration: 14 days
Dataset: 8,000 train + 2,000 validation (from expanded 10K)
Target: â‰¥98% test accuracy
Deliverable: GLADIATOR-SEC-8B-EXPERT v1.0
```

---

## APPENDIX: COMMANDS REFERENCE

### Network Setup (10GbE)
```bash
# ALPHA configuration
sudo ifconfig en[X] 10.0.10.1 netmask 255.255.255.0 up

# BETA configuration  
sudo ifconfig en[X] 10.0.10.2 netmask 255.255.255.0 up

# Test connectivity
ping 10.0.10.2  # from ALPHA
ping 10.0.10.1  # from BETA

# Throughput test (server on BETA, client on ALPHA)
# BETA: iperf3 -s
# ALPHA: iperf3 -c 10.0.10.2 -t 60 -P 4
```

### Dataset Monitoring
```bash
# Check generation progress on BETA
ssh beta.local "ls -1 /Volumes/DATA/GLADIATOR/datasets/expansion/*.jsonl | wc -l"

# Count samples
ssh beta.local "wc -l /Volumes/DATA/GLADIATOR/datasets/expansion/*.jsonl"

# Quality check
ssh beta.local "head -10 /Volumes/DATA/GLADIATOR/datasets/expansion/privilege_escalation_batch1.jsonl"
```

### Infrastructure Validation
```bash
# Storage check
df -h /Users/arthurdell/GLADIATOR

# GPU availability
python3 -c "import mlx.core as mx; print(f'GPU: {mx.metal.is_available()}, Cores: 80')"

# Database status
PGPASSWORD='Power$$336633$$' psql -U postgres -d aya_rag -c "SELECT current_phase, current_week FROM gladiator_project_state WHERE is_current = true;"
```

---

**Week 1 Plan Status**: âœ… READY TO EXECUTE  
**Prerequisites**: âœ… Week 0 GO decision achieved  
**Timeline**: 7 days (October 23-29, 2025)  
**Next Review**: End of Day 1 (network installation)

---

**END OF WEEK 1 EXECUTION PLAN**

